---
title: "DeepSeek OCR 实操和论文学习"
date: "2025-11-08T04:25:00.000Z"
lastmod: "2025-11-11T02:06:00.000Z"
draft: false
featuredImage: "https://aha.qingy.ing/api?page_id=2a52875f-4bfe-80a1-a7f1-f4a55ca7e3be"
series: []
authors:
  - "Hongbo Mo"
tags:
  - "AI"
categories:
  - "技术"
NOTION_METADATA:
  object: "page"
  id: "2a52875f-4bfe-80a1-a7f1-f4a55ca7e3be"
  created_time: "2025-11-08T04:25:00.000Z"
  last_edited_time: "2025-11-11T02:06:00.000Z"
  created_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  last_edited_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  cover:
    type: "file"
    file:
      url: "https://prod-files-secure.s3.us-west-2.amazonaws.com/dc681554-1505-4cec-9\
        a8f-844b66d5dcc8/677dadc0-97a7-4bcc-882a-6a6163144f74/DSCF8235_preview.\
        jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAY\
        LOAD&X-Amz-Credential=ASIAZI2LB466SBZ6MCEH%2F20251112%2Fus-west-2%2Fs3%\
        2Faws4_request&X-Amz-Date=20251112T014154Z&X-Amz-Expires=3600&X-Amz-Sec\
        urity-Token=IQoJb3JpZ2luX2VjEGIaCXVzLXdlc3QtMiJGMEQCIETfr2IEx2kPW9b05QC\
        %2F2ruCXG3RsDRMBaeoTCeMrlflAiAY%2FcxxnOWuxrhp53sRNUwbCUCqRw4Vs7jwsaau9z\
        4Rqir%2FAwgqEAAaDDYzNzQyMzE4MzgwNSIMfpuz3wBJqcKTQU7BKtwD8UmvEMb1B9ztTcz\
        6UtISvD%2FIKv2nSgsSY4OS%2B9oJIiA%2F4VygODmIyjUeWvRe4ThcWp6hTjGZHcf0Psnb\
        wknQSgWuI6kgbF8w0bhQ%2FwabczUfO7t1cgmear%2BmU3ZdxJXIyFHR68BFU6mHSnwyB%2\
        Ffd8rF52hPF6YYV9eMAguhSgqCe64pBJLHJ5Cc%2Fksq3WhpQmt9uXdXtnsz0x5WNknMI76\
        ATBjdpY2EjuNeT0%2BLkf8t6F%2BXl6HSUGV0X5XSaoSlT8atPZITeqeG32WcP3PIHY7s6O\
        %2F7eVhFIbKx2fCLgaXzn4aqMzx9Z29BnrydOhPfJZ11rEpSOs7yXwa40CdHTXknG9Ob3JF\
        WxsaK%2F2fRmIh2SrT3jIf7jrbr45eHv80gzSzwEY48Lbk2DyH1DdQeSbgFIOY%2FayZ%2F\
        d0a7cSPpXGWpum1SF%2FBArIDTcW6txa0izkRgXrFMbNViL1%2FU9cWWDScpRD7ZTdX0QAj\
        kN5AVq1ZUplDsJkFG%2BchLKB4%2Bte5S5ba56EJ%2B%2FBzYj975gzPI2U0k3V1EqP9CUv\
        1crZT3sA3tyuW9eW7fpMLeMzDHhcuvXYRe04EsgL6FiXKp1P9%2Fh9Q%2F4EkGpvejXhSu2\
        K0zz2bqT%2FrSU%2FU4OJwADPW4wr8HPyAY6pgFku0ItTqLVZH7otZvKTGGeT8nnYO3jiVV\
        l2bBohz7BCB2T5Jpuqb%2BzHXzisvsx6nJ%2BMFA1py4SJMLqZisZ0jQV4Ud6jMZT8MqgPU\
        vjPB5x%2FTh0CPa9iX91xe0c0a3xcCToJkbkm6Qvo6h%2BuIncdn8assnSGjmwJBM46Jf8l\
        42uLhYVjgr66QaXzqE7vEy6G5xryJsgnDPeni1jyCznhflsO6uRepkV&X-Amz-Signature\
        =2e7f2118e6adb67567e7e48fce0c36cd870e27a64289dfed69732e295dca0dea&X-Amz\
        -SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"
      expiry_time: "2025-11-12T02:41:54.678Z"
  icon: null
  parent:
    type: "database_id"
    database_id: "2742875f-4bfe-8102-8a94-f918ade28f0e"
  archived: false
  in_trash: false
  is_locked: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select: []
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    authors:
      id: "bK%3B%5B"
      type: "people"
      people:
        - object: "user"
          id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
          name: "Hongbo Mo"
          avatar_url: "https://lh3.googleusercontent.com/-TqDAswHjpLU/AAAAAAAAAAI/AAAAAAA\
            ACcE/ytljzmTe0FE/photo.jpg"
          type: "person"
          person:
            email: "zjutpolym@gmail.com"
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select:
        - id: "40547cf7-b44a-442e-bda4-0c8227541945"
          name: "AI"
          color: "purple"
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select:
        - id: "3e64a788-1d3d-460d-ad3a-18dc5cbc48e5"
          name: "技术"
          color: "brown"
    Last edited time:
      id: "vbGE"
      type: "last_edited_time"
      last_edited_time: "2025-11-11T02:06:00.000Z"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text: []
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "DeepSeek OCR 实操和论文学习"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "DeepSeek OCR 实操和论文学习"
          href: null
  url: "https://www.notion.so/DeepSeek-OCR-2a52875f4bfe80a1a7f1f4a55ca7e3be"
  public_url: "https://polym.notion.site/DeepSeek-OCR-2a52875f4bfe80a1a7f1f4a55ca7e3be"
MANAGED_BY_NOTION_HUGO: true

---


# 部署体验


以 Nvidia 4090 + Nvidia-driver-580 驱动为例：


```bash
git clone https://github.com/deepseek-ai/DeepSeek-OCR.git && cd DeepSeek-OCR

conda create -n ocr python=3.12
conda install cuda-toolkit=12.8

pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirement.py

python DeepSeek-OCR-master/DeepSeek-OCR-hf/run_dpsk_ocr.py
```


也可以直接体验：[https://huggingface.co/spaces/merterbak/DeepSeek-OCR-Demo](https://huggingface.co/spaces/merterbak/DeepSeek-OCR-Demo)


![](https://aha.qingy.ing/api?block_id=2a52875f-4bfe-80c8-af85-eb0a47753d5f)


# 模型架构


![](https://aha.qingy.ing/api?block_id=2a52875f-4bfe-8065-b388-edcf67124cca)


总体包含两个部分「DeepEncoder」和「LLM」。其中「DeepEncoder」由 「SAM」+「CLIP」模型结构组成，而「LLM」使用的是 DeepSeek-3B。


模型推理过程如下：

1. 将输入的图片进行切分，得到 n 个 16x16 patches 图片块
1. 使用 SAM 模型获取局部的 attention，并采用 16x 的下采样将 vision token 压缩到 n/16 个
1. 将 vision token 逐个输入到 CLIP 模型，得到图片块的 Embedding
1. 将图片块的 Embedding 和 Prompt 对应的 Embedding 合并作为输入，喂给 DeepSeek-3B
1. DeepSeek-3B 根据输入语义，完成实际的任务

# Q&A 自问自答


### Q1. DeepSeek-3B 作为 backbone 时参数是否冻结？


没有冻结。无论是 SAM、CLIP、DeepSeek-3B 的所有参数均可以调整。


### Q2. SAM/CLIP/DeepSeek-3B 间的维度如何对齐？


利用卷积层和投影层。

- 在 SAM ⇒ CLIP 中间有一个 16x 的卷积层，可以特征向量缩放到 1024。这与 CLIP 模型的特征向量维度一致。
- 在 CLIP ⇒ DeepSeek-3B 中间有一个投影层，会将特征向量维度缩放到 1280。值得注意的是，在投影层前会将 SAM 输出的 1024 维特征向量与 CLIP 输出的 1024 维特征向量进行拼接，然后再进行投影。

```bash
n_embed = 1280  # 目标输出维度
self.projector = MlpProjector(Dict(
    projector_type="linear", 
    input_dim=2048,      # 输入维度: CLIP + SAM = 2048
    n_embed=n_embed      # 输出维度: 1280
))
```


### Q3. Vision token 和 Prompt 如何作为 DeepSeek-3B 的 input？


首先 Vision tokens 和 Prompt 对应的 Embedding 都是 1280 维度的，一个是视觉的 Embedding，另一个是文本的 Embedding。由于使用 CLIP 模型，会让视觉 Embedding 的分布与文本 Embedding 的分布尽可能对应。而对于 DeepSeek-3B 来说，输入的是语义，而不关心是视觉的 Embedding 还是文本的 Embedding。这个在 23 年的论文 NExT-GPT 首次被提出，如下所示：


![](https://aha.qingy.ing/api?block_id=2a82875f-4bfe-8073-ba05-f95b835d03fc)


# 几点畅想

1. 在论文中提到虽然训练数据集中包含了 100+ 种语言，但绝大多数还是中英文的资料。因此，在 DeepSeek OCR 基础上对不同的语言进行调优应该是有可能的。例如，藏文、梵文等。
1. OCR 和 LLM 结合，通过自然语言来驱动 OCR 处理任务，可以发挥 LLM 的泛化能力，为处理复杂任务提供了更多的可能性。
1. DeepSeek OCR 模型参数较少，在端侧也会有一些场景
1. 关于利用 Vision Token 来压缩 LLM 对话上下文长度，是一个很有意思的想法。是否会出现通用的 DeepEncoder 可以嫁接在任意 LLM 前面来处理信息压缩后的图片？

# 参考资料

1. 
1. [https://github.com/deepseek-ai/DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR)
1. 
1. 
