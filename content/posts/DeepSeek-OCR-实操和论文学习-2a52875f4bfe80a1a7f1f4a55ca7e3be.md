---
title: "DeepSeek OCR 实操和论文学习"
date: "2025-11-08T04:25:00.000Z"
lastmod: "2025-11-09T08:06:00.000Z"
draft: false
featuredImage: "https://aha.qingy.ing/api?page_id=2a52875f-4bfe-80a1-a7f1-f4a55ca7e3be"
series: []
authors:
  - "Hongbo Mo"
tags:
  - "AI"
categories:
  - "技术"
NOTION_METADATA:
  object: "page"
  id: "2a52875f-4bfe-80a1-a7f1-f4a55ca7e3be"
  created_time: "2025-11-08T04:25:00.000Z"
  last_edited_time: "2025-11-09T08:06:00.000Z"
  created_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  last_edited_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  cover:
    type: "file"
    file:
      url: "https://prod-files-secure.s3.us-west-2.amazonaws.com/dc681554-1505-4cec-9\
        a8f-844b66d5dcc8/677dadc0-97a7-4bcc-882a-6a6163144f74/DSCF8235_preview.\
        jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAY\
        LOAD&X-Amz-Credential=ASIAZI2LB466ZSG2F5K3%2F20251110%2Fus-west-2%2Fs3%\
        2Faws4_request&X-Amz-Date=20251110T014547Z&X-Amz-Expires=3600&X-Amz-Sec\
        urity-Token=IQoJb3JpZ2luX2VjEC8aCXVzLXdlc3QtMiJHMEUCIQD7T89xUJgKsGvHwRM\
        jKCBlOdAPtXGVlAVjjDlRW9BlhAIgDcgEZsiGN1Miiku5nFZxg1NktXHGYVZfx%2F%2F31e\
        gVhqkqiAQI%2BP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw2Mzc0MjMxODM4MDUiDOB\
        e%2FDCN6%2FGHjXOn0yrcA4IRRnHbHOQh3lPuO5droHyvQsNqwqiP6FOSPRYApzdAa0Bv1a\
        NGKM9YwKBDukaV4hgmbUjEYP%2FEJMkQ6GAC9jv%2BXniEiewZPvpkdzRrG8uKUrRg5CU82\
        Vjsau66unhkDKdrJVdaDFMg7pus0rW7Ikm12KHZqCKtohmZu40pr7okfPNrg25nay67rVPq\
        B3HefrfR%2FvVPevpRzshAgkKm1Khjes8TfdE7AYtISQ47GSvDckjlhSxNFpLypxkVJnJ4O\
        TlicvBGRNXaBqAvH8o5FIuTRrueNXbKG3kaZqATYSdm5C6PQtMS8YfKtqY9oraxZOSjIT6m\
        Wo64Wz17OoAvmJSxp1pJ1CZcLdilGx6fEuC0m2AFdPZpJJQ7MqKIYUfNttCnWyjQF%2BRFC\
        Iq%2BoQFVT6b%2B9fA9o3yrEPRLsoShUr2r1bFm94nd2tjGXdBzUCHqNNX54RkEY7BuUaPC\
        5kq2V1rzVlL99DjLbyZO3%2BjMvit9AjlMUIYzBdDxU%2FcjUGleeREn3Nw4TkdhHYYN9nP\
        UJRFX75XIfgZfPRW%2F3QJbkM3LtrgrQf6ZI3Fzn63EADFb1JgWuCbyOEYM2NSZsocniE0f\
        QpbzAItvkHp2JTEdI4QNErRu7n3BsFvFl0lpMIy1xMgGOqUBy8cmyy8V4mV3%2FfPkQUHfk\
        Milm32gRwNvidILtyRPz%2BhackwZgSO3LmgsA%2BAml9R2VJ%2FIybXtzlQXoCk3PzFE2B\
        9S2tt2MR0vtGdZ4Qf%2Bvn43RPDg3KuII%2Fze3r5YJYaYd9D18S9C23NqPajFKmFMgHZta\
        ox%2BODC7DWySh0N0ejNvJenXHZnwVf236S4l4ZeTOi4%2B%2FLBLZXSflvABxvo1eSdPxW\
        vR&X-Amz-Signature=f9ab96fbdb7b007e2e87ebc03c41e6522b807c153345d4fd1fef\
        3cbc2bf02131&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=\
        GetObject"
      expiry_time: "2025-11-10T02:45:47.126Z"
  icon: null
  parent:
    type: "database_id"
    database_id: "2742875f-4bfe-8102-8a94-f918ade28f0e"
  archived: false
  in_trash: false
  is_locked: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select: []
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    authors:
      id: "bK%3B%5B"
      type: "people"
      people:
        - object: "user"
          id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
          name: "Hongbo Mo"
          avatar_url: "https://lh3.googleusercontent.com/-TqDAswHjpLU/AAAAAAAAAAI/AAAAAAA\
            ACcE/ytljzmTe0FE/photo.jpg"
          type: "person"
          person:
            email: "zjutpolym@gmail.com"
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select:
        - id: "40547cf7-b44a-442e-bda4-0c8227541945"
          name: "AI"
          color: "purple"
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select:
        - id: "3e64a788-1d3d-460d-ad3a-18dc5cbc48e5"
          name: "技术"
          color: "brown"
    Last edited time:
      id: "vbGE"
      type: "last_edited_time"
      last_edited_time: "2025-11-09T08:06:00.000Z"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text: []
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "DeepSeek OCR 实操和论文学习"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "DeepSeek OCR 实操和论文学习"
          href: null
  url: "https://www.notion.so/DeepSeek-OCR-2a52875f4bfe80a1a7f1f4a55ca7e3be"
  public_url: "https://polym.notion.site/DeepSeek-OCR-2a52875f4bfe80a1a7f1f4a55ca7e3be"
MANAGED_BY_NOTION_HUGO: true

---


# 部署体验


以 Nvidia 4090 + Nvidia-driver-580 驱动为例：


```bash
git clone https://github.com/deepseek-ai/DeepSeek-OCR.git && cd DeepSeek-OCR

conda create -n ocr python=3.12
conda install cuda-toolkit=12.8

pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirement.py

python DeepSeek-OCR-master/DeepSeek-OCR-hf/run_dpsk_ocr.py
```


也可以直接体验：[https://huggingface.co/spaces/merterbak/DeepSeek-OCR-Demo](https://huggingface.co/spaces/merterbak/DeepSeek-OCR-Demo)


![](https://aha.qingy.ing/api?block_id=2a52875f-4bfe-80c8-af85-eb0a47753d5f)


# 模型架构


![](https://aha.qingy.ing/api?block_id=2a52875f-4bfe-8065-b388-edcf67124cca)


总体包含两个部分「DeepEncoder」和「LLM」。其中「DeepEncoder」由 「SAM」+「CLIP」模型结构组成，而「LLM」使用的是 DeepSeek-3B。


模型推理过程如下：

1. 将输入的图片进行切分，得到 n 个 16x16 patches 图片块
1. 使用 SAM 模型获取局部的 attention，并采用 16x 的下采样将 vision token 压缩到 n/16 个
1. 将 vision token 逐个输入到 CLIP 模型，得到图片块的 Embedding
1. 将图片块的 Embedding 和 Prompt 对应的 Embedding 合并作为输入，喂给 DeepSeek-3B
1. DeepSeek-3B 根据输入语义，完成实际的任务

# Q&A 自问自答


### Q1. DeepSeek-3B 作为 backbone 时参数是否冻结？


没有冻结。无论是 SAM、CLIP、DeepSeek-3B 的所有参数均可以调整。


### Q2. SAM/CLIP/DeepSeek-3B 间的维度如何对齐？


利用卷积层和投影层。

- 在 SAM ⇒ CLIP 中间有一个 16x 的卷积层，可以特征向量缩放到 1024。这与 CLIP 模型的特征向量维度一致。
- 在 CLIP ⇒ DeepSeek-3B 中间有一个投影层，会将特征向量维度缩放到 1280。值得注意的是，在投影层前会将 SAM 输出的 1024 维特征向量与 CLIP 输出的 1024 维特征向量进行拼接，然后再进行投影。

```bash
n_embed = 1280  # 目标输出维度
self.projector = MlpProjector(Dict(
    projector_type="linear", 
    input_dim=2048,      # 输入维度: CLIP + SAM = 2048
    n_embed=n_embed      # 输出维度: 1280
))
```


### Q3. Vision token 和 Prompt 如何作为 DeepSeek-3B 的 input？


首先 Vision tokens 和 Prompt 对应的 Embedding 都是 1280 维度的，一个是视觉的 Embedding，另一个是文本的 Embedding。由于使用 CLIP 模型，会让视觉 Embedding 的分布与文本 Embedding 的分布尽可能对应。而对于 DeepSeek-3B 来说，输入的是语义，而不关心是视觉的 Embedding 还是文本的 Embedding。


# 几点畅想

1. 在论文中提到虽然训练数据集中包含了 100+ 种语言，但绝大多数还是中英文的资料。因此，在 DeepSeek OCR 基础上对不同的语言进行调优应该是有可能的。例如，藏文、梵文等。
1. OCR 和 LLM 结合，通过自然语言来驱动 OCR 处理任务，可以发挥 LLM 的泛化能力，为处理复杂任务提供了更多的可能性。
1. DeepSeek OCR 模型参数较少，在端侧也会有一些场景
1. 关于利用 Vision Token 来压缩 LLM 对话上下文长度，是一个很有意思的想法。是否会出现通用的 DeepEncoder 可以嫁接在任意 LLM 前面来处理信息压缩后的图片？

# 参考资料

1. 
1. [https://github.com/deepseek-ai/DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR)
1. 
