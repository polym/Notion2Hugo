---
title: "fp8 量化的几种姿势"
date: "2025-10-13T12:22:00.000Z"
lastmod: "2025-10-14T14:46:00.000Z"
draft: false
featuredImage: "https://aha.qingy.ing/api?page_id=28b2875f-4bfe-802b-aceb-e6b06e48c058"
series: []
authors:
  - "Hongbo Mo"
tags:
  - "AI"
  - "PyTorch"
categories:
  - "技术"
NOTION_METADATA:
  object: "page"
  id: "28b2875f-4bfe-802b-aceb-e6b06e48c058"
  created_time: "2025-10-13T12:22:00.000Z"
  last_edited_time: "2025-10-14T14:46:00.000Z"
  created_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  last_edited_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  cover:
    type: "file"
    file:
      url: "https://prod-files-secure.s3.us-west-2.amazonaws.com/dc681554-1505-4cec-9\
        a8f-844b66d5dcc8/26c13cd4-5fdd-42b4-b999-932c1238eec9/dafosi.jpeg?X-Amz\
        -Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz\
        -Credential=ASIAZI2LB466RVRJMYTC%2F20251015%2Fus-west-2%2Fs3%2Faws4_req\
        uest&X-Amz-Date=20251015T013758Z&X-Amz-Expires=3600&X-Amz-Security-Toke\
        n=IQoJb3JpZ2luX2VjEMH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHM\
        EUCIQCSKCMPrdvlL%2BhpdKssqWGOdOKVBulUs%2FnVMSiTPOyTKAIgFnJ7TKLsVf97wKDJ\
        cP4bMAoeKJmGT5vqU3gY2NgtvUsq%2FwMIahAAGgw2Mzc0MjMxODM4MDUiDC2BY8ZliWpXO\
        A4b8ircA4DUDsQM9ZkKdiQAe2AHftokVUw6LFOZMNu1Zbo2ml2erUe4n7SvwOP80oTTLAm%\
        2FwfRg2wBkfN%2FaAQtp3%2BscSGF%2F24aVfXaPhrpMi250d3Y1HWHDr6JuBjMyxLMS3d7\
        tF5K0aUHZQAiGvvjU2HUwPTnItDfD76BeTHiG5sYw3YHvCUruEqb1sD4TbM672Oeru835sS\
        ohIodUTHUhzf1%2FeDAVICEYdGKivY7gWphYNJ%2FbvjKqQ4QnZczT6PUPJA1lCzqWhR8aq\
        gRqb9ZJid4elsV6wIx8Uh1rFiGbs8%2BMTP9TO013LfeYbZB%2BqOiV8QsxZKiQK69SrNSk\
        6hxYjr86pBqyJGP0hAWfVbI7LvavgSDNYB%2BMhD8leNHDrfZa1SRmSVN95zwoULQCU78l5\
        2d1yLFxxi60XnEvFqoNKgdTikrSg5Yg7d1k%2FmMue%2FSLyaRIxWRLe12Fj7Na0Qm1tR3Q\
        GJE4EvyWsjlljSfPQ6LvguoMkhLSN4qpQo%2F4bGe%2FF15a%2BB3dx49MWtOsvV9z8xN0c\
        v2PQr6CVL8z5%2F0O%2BpdfWXsw7CWYDf1pfCnKq9ORbzyMmiN4caupGtDnaL2cb46dGvYH\
        1W2z2%2BONDxr0uEgvgah4%2BXvreQvPeAbTRbGzMPzbu8cGOqUBNnXqckoSTOOicyVyXGy\
        503v8Ym9P1OqwTTJoLnOOF8eq%2B7bINKvB81pId5bdNq9d3dq1K0ATw02PUKz9m96%2BQC\
        Lo5P5Domg1bxfqwYlBB1f%2FQUfqeClu5lZIUDWQIj2IqBOs9jVPPB%2FQrK1vFUEFHtCwj\
        KESqmLj9j3kxGHuZ%2BOF3dm3ZAFrjo%2BzXfw8%2BJPDynYq5%2BR%2BarUeAnM%2F2tVN\
        LwlJBM4w&X-Amz-Signature=494f935c7ed56e967b28ec101af2ed01d09686059f756b\
        2030615435eafb52bf&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED\
        &x-id=GetObject"
      expiry_time: "2025-10-15T02:37:58.502Z"
  icon: null
  parent:
    type: "database_id"
    database_id: "2742875f-4bfe-8102-8a94-f918ade28f0e"
  archived: false
  in_trash: false
  is_locked: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select: []
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    authors:
      id: "bK%3B%5B"
      type: "people"
      people:
        - object: "user"
          id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
          name: "Hongbo Mo"
          avatar_url: "https://lh3.googleusercontent.com/-TqDAswHjpLU/AAAAAAAAAAI/AAAAAAA\
            ACcE/ytljzmTe0FE/photo.jpg"
          type: "person"
          person:
            email: "zjutpolym@gmail.com"
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select:
        - id: "40547cf7-b44a-442e-bda4-0c8227541945"
          name: "AI"
          color: "purple"
        - id: "683854ba-d6f3-4e29-a95e-585859c16642"
          name: "PyTorch"
          color: "orange"
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select:
        - id: "3e64a788-1d3d-460d-ad3a-18dc5cbc48e5"
          name: "技术"
          color: "brown"
    Last edited time:
      id: "vbGE"
      type: "last_edited_time"
      last_edited_time: "2025-10-14T14:46:00.000Z"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text: []
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "fp8 量化的几种姿势"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "fp8 量化的几种姿势"
          href: null
  url: "https://www.notion.so/fp8-28b2875f4bfe802bacebe6b06e48c058"
  public_url: "https://polym.notion.site/fp8-28b2875f4bfe802bacebe6b06e48c058"
MANAGED_BY_NOTION_HUGO: true

---


# 背景


今年以来，在图像视频领域，开源的 AIGC 模型参数规模越来越大，例如，QwenImage 达到了 20B，Wan2.2 达到了 27B（High-Noise 和 Low-Noise 专家各占用 14B）。参数规模越大，就意味着需要更多的显存来加载模型，需要更多的算力来进行模型推理。而此时如果你想将这些 SOTA 模型运行在更加经济的消费级显卡时，就需要对模型进行量化（可以理解为压缩）。对于一张 24GB 显存的 4090 显卡而言，如果想要加载 QwenImage 20B 模型，就必须将模型权重量化到 fp8 精度。具体估算方式如下：


```python
num_params = 20 * 2^30
fp8_size = 1 byte

total_size = num_params * fp8_size = 20 * 2^30 * 1 byte = 20 GBytes
```


最近 musubi-tuner 的作者正在比较不同 fp8 量化方式对图片生成结果的影响，正好我对 ComfyUI 模型使用的 fp8_e4m3、fp8_e4m3fn、fp8_e5m2 等不同的 fp8 精度也一知半解，于是决定做一次系统的学习与整理。


# 关于精度与量化


![](https://aha.qingy.ing/api?block_id=28b2875f-4bfe-80ed-bc65-ca7c746b0ad1)


浮点精度相信大家都不陌生，只不过接触比较多可能是 float32 单精度浮点数或者 float64 双精度浮点数。简单来说，浮点数的表示分为「符号位」、「指数位」、「尾数位」三个部分，如上图所示，其中每个色块代表一个 bit。你会发现 float8 精度有两个表示方式，fp8_e4m3 和 fp8_e5m2，不同的表示意味着不同的取值范围。


而所谓的量化，其实是将数值从较大的精度范围（bf16）缩小到较小的精度范围（fp8_e4m3）；反量化则刚好相反。


另外，可能还会看到 `fp8_e4m3fn`、`fp8_e4m3fnuz` 等精度，其中 `fn` 代表没有无穷值，`uz` 代表没有负零。


# FP8 的几种量化方式


![](https://aha.qingy.ing/api?block_id=28c2875f-4bfe-80fe-a050-e131b6fe277d)


将 bf16 精度的数值量化到 fp8_e5m2 精度，有以下 3 种方式：

1. Cast-To：该方法不属于量化范畴，只是简单的数值转换。可以发现灰色部分会被直接映射到 INF 或者 -INF 上。代码验证：

	```shell
	>>> v = torch.finfo(torch.bfloat16).max
	>>> v
	3.3895313892515355e+38
	>>> tv=torch.Tensor(v)
	>>> tv.to(torch.float8_e5m2)
	tensor([inf], dtype=torch.float8_e5m2)
	```

1. Tensor-wise FP8 量化：对于每个 Tensor（模型是由多个 Tensor 组成，Tensor 中包含一组参数），找出当前参数数值的最小取值范围，并将这个范围映射到 fp8 的范围内。该过程就是量化，其中映射过程会有一个缩放因子，通过缩放因子可以对参数进行反量化。
1. Block-wise FP8 量化：将每个 Tensor 按照固定 block 大小切分成多个 block，在对 block 中的参数按 Tensor-wise 的方法进行量化，区别在于每个 block 会有一个缩放因子。这个方法粒度更细，精度相对会更高。

⚠️ 图中不同精度的范围为粗略值，仅展示使用。


# Tensor-wise FP8 量化模型布局比较


![](https://aha.qingy.ing/api?block_id=28b2875f-4bfe-8020-b8e2-e4701e88442e)


在 Kijai 大神量化的 fp8_scale 模型中，可以看到每个 weight 同级新增了 scale_weight 用于记录缩放因子。


# 细粒度量化 (Fine-grained quantization)


> HuggingFace transformers 库中提供了对 block-wise fp8 量化方式的支持


![](https://aha.qingy.ing/api?block_id=28b2875f-4bfe-80fb-9ad8-cbb92ac6dc26)


# 参考资料

- 
- [https://huggingface.co/docs/transformers/en/quantization/finegrained_fp8](https://huggingface.co/docs/transformers/en/quantization/finegrained_fp8)
- [https://www.aidoczh.com/onnx/technical/float8.html](https://www.aidoczh.com/onnx/technical/float8.html)
- 
- 
