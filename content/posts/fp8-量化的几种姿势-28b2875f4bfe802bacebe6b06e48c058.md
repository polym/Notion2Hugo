---
title: "fp8 量化的几种姿势"
date: "2025-10-13T12:22:00.000Z"
lastmod: "2025-10-15T03:45:00.000Z"
draft: false
featuredImage: "https://aha.qingy.ing/api?page_id=28b2875f-4bfe-802b-aceb-e6b06e48c058"
series: []
authors:
  - "Hongbo Mo"
tags:
  - "AI"
  - "PyTorch"
categories:
  - "技术"
NOTION_METADATA:
  object: "page"
  id: "28b2875f-4bfe-802b-aceb-e6b06e48c058"
  created_time: "2025-10-13T12:22:00.000Z"
  last_edited_time: "2025-10-15T03:45:00.000Z"
  created_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  last_edited_by:
    object: "user"
    id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
  cover:
    type: "file"
    file:
      url: "https://prod-files-secure.s3.us-west-2.amazonaws.com/dc681554-1505-4cec-9\
        a8f-844b66d5dcc8/26c13cd4-5fdd-42b4-b999-932c1238eec9/dafosi.jpeg?X-Amz\
        -Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz\
        -Credential=ASIAZI2LB466ZHOTDEH2%2F20251016%2Fus-west-2%2Fs3%2Faws4_req\
        uest&X-Amz-Date=20251016T013658Z&X-Amz-Expires=3600&X-Amz-Security-Toke\
        n=IQoJb3JpZ2luX2VjENr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGM\
        EQCIFytpqLjI6Q9FnUbB0KHkxxkCTTMiGk81bP9s0x%2BNjKxAiAnqTEvtSbPfej3Ic8E5G\
        dfWsathXbRt4aqsbRrJ2T2DyqIBAiC%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzN\
        zQyMzE4MzgwNSIMs0GT3tWLho%2Bp6czQKtwDEZePICCXTr2FR5rgZGEoDTHQ9zblkO9L%2\
        Bwa6uq1EJ5d5jYOSG6352oY36LhauQBYYZgKQeJVyJ5uTzIEyMZDrdJ6EHO%2Ftoa2anwPm\
        w7t3FB3BdS%2Bd635fHemVFbGD5WsvYoFTbhjvR1KBOemnNYp5tVdDA6SfZR1%2BCAwGx2x\
        3INohueeP4eJmcqE8CJeEfUbqrTmRiXcrUMCTnuQEzQ8QMI4X4c9jM0BbiEXG%2BC3lxL4N\
        5Uf6Dp4TAN6jROUEQefqA1zwynDgtT7SWNGxClo9d5MwrPHDlyB3aiH1p8tA%2Fameq8JGg\
        fxBJPJB7nCvZ7R14YFx75tGecqBE5k7oq1I8Eu%2BANnK%2FadbodImPqr7vqTyoR01OngB\
        st1Aif6WczYFih5babUry0pk6lsavW6BBb5AK6FlRMm%2BXv54wRZPwlWBGW89wBHERg9Sk\
        7lnNEofKxf81s5ZM4gYwdmIz9C9VgMg9dfWUzy9sNg03IO48k5exmrGFQG3gU5btYfdZ29j\
        gFl3XwVXW1uKiSrxm%2Ftmj1s5PEvDcI3kl5NQqpdBSFtPUOt9KC5GYj3I4nm2IB6xrf6Pg\
        ZOTp6pLoudYTEnLcIrKjOo2pixMMdZf%2FcX4D9jGezxlBl%2Fo0xh23cwipLBxwY6pgHwj\
        ZLjmEFvStaAGtJKXGhUYLwUsQU5tuSFdBSP2HA%2FuoWqovKXEslzDTwQYto2Wk5ly%2Bq4\
        Jk0sUt4TiuSLIMGFFHKo%2Bqak7kHkOa706EEo9UKc4UeZoOCxw9jx0in5VN4xEDRSc4oke\
        ILaxxzkZiQCjLLpD6uoB4aWF27JJnn0RkzZayZdIgW%2BX3%2BFPNc1hIlXHSKcXQMvrQZf\
        KpNIIdH8vQAJD5M2&X-Amz-Signature=aa1b97e4308795384bb4b7c7d2216d4dd96517\
        182d1c55c19c53b31c89305ab7&X-Amz-SignedHeaders=host&x-amz-checksum-mode\
        =ENABLED&x-id=GetObject"
      expiry_time: "2025-10-16T02:36:58.126Z"
  icon: null
  parent:
    type: "database_id"
    database_id: "2742875f-4bfe-8102-8a94-f918ade28f0e"
  archived: false
  in_trash: false
  is_locked: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select: []
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    authors:
      id: "bK%3B%5B"
      type: "people"
      people:
        - object: "user"
          id: "1b106df3-cc7b-493e-9afa-a6a7c977ec1b"
          name: "Hongbo Mo"
          avatar_url: "https://lh3.googleusercontent.com/-TqDAswHjpLU/AAAAAAAAAAI/AAAAAAA\
            ACcE/ytljzmTe0FE/photo.jpg"
          type: "person"
          person:
            email: "zjutpolym@gmail.com"
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select:
        - id: "40547cf7-b44a-442e-bda4-0c8227541945"
          name: "AI"
          color: "purple"
        - id: "683854ba-d6f3-4e29-a95e-585859c16642"
          name: "PyTorch"
          color: "orange"
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select:
        - id: "3e64a788-1d3d-460d-ad3a-18dc5cbc48e5"
          name: "技术"
          color: "brown"
    Last edited time:
      id: "vbGE"
      type: "last_edited_time"
      last_edited_time: "2025-10-15T03:45:00.000Z"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text: []
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "fp8 量化的几种姿势"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "fp8 量化的几种姿势"
          href: null
  url: "https://www.notion.so/fp8-28b2875f4bfe802bacebe6b06e48c058"
  public_url: "https://polym.notion.site/fp8-28b2875f4bfe802bacebe6b06e48c058"
MANAGED_BY_NOTION_HUGO: true

---


# 背景


今年以来，在图像视频领域，开源的 AIGC 模型参数规模越来越大，例如，QwenImage 达到了 20B，Wan2.2 达到了 27B（High-Noise 和 Low-Noise 专家各占用 14B）。参数规模越大，就意味着需要更多的显存来加载模型，需要更多的算力来进行模型推理。而此时如果你想将这些 SOTA 模型运行在更加经济的消费级显卡时，就需要对模型进行量化（可以理解为压缩）。对于一张 24GB 显存的 4090 显卡而言，如果想要加载 QwenImage 20B 模型，就必须将模型权重量化到 fp8 精度。具体估算方式如下：


```python
num_params = 20 * 2^30
fp8_size = 1 byte

total_size = num_params * fp8_size = 20 * 2^30 * 1 byte = 20 GBytes
```


最近 musubi-tuner 的作者正在比较不同 fp8 量化方式对图片生成结果的影响，正好我对 ComfyUI 模型使用的 fp8_e4m3、fp8_e4m3fn、fp8_e5m2 等不同的 fp8 精度也一知半解，于是决定做一次系统的学习与整理。


# 关于精度与量化


![](https://aha.qingy.ing/api?block_id=28b2875f-4bfe-80ed-bc65-ca7c746b0ad1)


浮点精度相信大家都不陌生，只不过接触比较多可能是 float32 单精度浮点数或者 float64 双精度浮点数。简单来说，浮点数的表示分为「符号位」、「指数位」、「尾数位」三个部分，如上图所示，其中每个色块代表一个 bit。你会发现 float8 精度有两个表示方式，fp8_e4m3 和 fp8_e5m2，不同的表示意味着不同的取值范围。


而所谓的量化，其实是将数值从较大的精度范围（bf16）缩小到较小的精度范围（fp8_e4m3）；反量化则刚好相反。


另外，可能还会看到 `fp8_e4m3fn`、`fp8_e4m3fnuz` 等精度，其中 `fn` 代表没有无穷值，`uz` 代表没有负零。


# FP8 的几种量化方式


![](https://aha.qingy.ing/api?block_id=28c2875f-4bfe-80fe-a050-e131b6fe277d)


将 bf16 精度的数值量化到 fp8_e5m2 精度，有以下 3 种方式：

1. Cast-To：该方法不属于量化范畴，只是简单的数值转换。可以发现灰色部分会被直接映射到 INF 或者 -INF 上。代码验证：

	```shell
	>>> v = torch.finfo(torch.bfloat16).max
	>>> v
	3.3895313892515355e+38
	>>> tv=torch.Tensor(v)
	>>> tv.to(torch.float8_e5m2)
	tensor([inf], dtype=torch.float8_e5m2)
	```

1. Tensor-wise FP8 量化：对于每个 Tensor（模型是由多个 Tensor 组成，Tensor 中包含一组参数），找出当前参数数值的最小取值范围，并将这个范围映射到 fp8 的范围内。该过程就是量化，其中映射过程会有一个缩放因子，通过缩放因子可以对参数进行反量化。
1. Block-wise FP8 量化：将每个 Tensor 按照固定 block 大小切分成多个 block，在对 block 中的参数按 Tensor-wise 的方法进行量化，区别在于每个 block 会有一个缩放因子。这个方法粒度更细，精度相对会更高。

⚠️ 图中不同精度的范围为粗略值，仅展示使用。


# Tensor-wise FP8 量化模型布局比较


![](https://aha.qingy.ing/api?block_id=28b2875f-4bfe-8020-b8e2-e4701e88442e)


在 Kijai 大神量化的 fp8_scale 模型中，可以看到每个 weight 同级新增了 scale_weight 用于记录缩放因子。


# 细粒度量化 (Fine-grained quantization)


> HuggingFace transformers 库中提供了对 block-wise fp8 量化方式的支持


![](https://aha.qingy.ing/api?block_id=28b2875f-4bfe-80fb-9ad8-cbb92ac6dc26)


# 参考资料

- 
- [https://huggingface.co/docs/transformers/en/quantization/finegrained_fp8](https://huggingface.co/docs/transformers/en/quantization/finegrained_fp8)
- [https://www.aidoczh.com/onnx/technical/float8.html](https://www.aidoczh.com/onnx/technical/float8.html)
- 
- 
- [https://developer.nvidia.com/zh-cn/blog/fp8-challenges-best-practices/](https://developer.nvidia.com/zh-cn/blog/fp8-challenges-best-practices/)
